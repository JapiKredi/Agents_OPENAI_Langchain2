{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjSx2D7x7u4Oqd2xNIDYaI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JapiKredi/Agents_OPENAI_Langchain2/blob/main/Agents_OPENAI_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsec7O3Ea6pF",
        "outputId": "37acfa8e-5e20-488b-8ce3-39f7c03aa018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-community langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "id": "1QqNZJ1ScDl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "-aL3D36-cIok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY']=OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "Txcbv-vUcLDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "-IeJQDB4cQJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatOpenAI(model=\"gpt-3.5-turbo-1106\")"
      ],
      "metadata": {
        "id": "5yKEtjoacSm0"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage"
      ],
      "metadata": {
        "id": "LA37ZUylcVVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "id": "45Q_wd2hcbIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"you are a helpful assistant. Answer all the question to the best of your ability.\"\n",
        "    ),\n",
        "    MessagesPlaceholder(variable_name = \"messages\"),\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "peTWHAVNcdUX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "AdZZHjAtc9M6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(\n",
        "    {\n",
        "       \"messages\": [\n",
        "\n",
        "                   HumanMessage(\n",
        "                       content= \"translate this given sentence from english to french: I LOVE AI.\"\n",
        "                   ),\n",
        "\n",
        "                   AIMessage(content= \"J'adore la AI.\"),\n",
        "                   HumanMessage(content= \"what did you just say?\"),\n",
        "\n",
        "       ] ,\n",
        "\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "SAaEy5r0dK9t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "9FM4Ra-ceUUv",
        "outputId": "efc6a217-2d1f-4b82-d9f1-5c2147952963"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I said \"I love AI\" in French.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory"
      ],
      "metadata": {
        "id": "2AdPOX21eeMk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history=ChatMessageHistory()"
      ],
      "metadata": {
        "id": "ItqaEbnbgcU1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_user_message(\"translate this given sentence from english to french: I LOVE AI.\")"
      ],
      "metadata": {
        "id": "HqEw9_RigekM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_ai_message(\"J'adore la AI.\")"
      ],
      "metadata": {
        "id": "mBllurUMgq5m"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = \"who is the prime minister of india\""
      ],
      "metadata": {
        "id": "j1qnP085guZt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_user_message(input1)"
      ],
      "metadata": {
        "id": "QWIybwI6g61l"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDXMl3cVhA88",
        "outputId": "b93b99a3-33ca-4d18-f54c-2f60e0adf1d0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='translate this given sentence from english to french: I LOVE AI.'),\n",
              " AIMessage(content=\"J'adore la AI.\"),\n",
              " AIMessage(content=\"J'adore la AI.\"),\n",
              " HumanMessage(content='who is the prime minister of india')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_chat_history.messages\n",
        "    }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ed_036h2hGYb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "JiGXefP0hOGz",
        "outputId": "c8032adb-b921-4528-d4cd-d33e0cbf0032"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As of now, the Prime Minister of India is Narendra Modi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_ai_message(response)"
      ],
      "metadata": {
        "id": "mH94v1gPhRFo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg4t4yiHhbkT",
        "outputId": "7f695f6d-fa9e-48b8-c62c-ed77796b7156"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='translate this given sentence from english to french: I LOVE AI.'),\n",
              " AIMessage(content=\"J'adore la AI.\"),\n",
              " AIMessage(content=\"J'adore la AI.\"),\n",
              " HumanMessage(content='who is the prime minister of india'),\n",
              " AIMessage(content='As of now, the Prime Minister of India is Narendra Modi.'),\n",
              " AIMessage(content='As of now, the Prime Minister of India is Narendra Modi.', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 74, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_3d37c73133', 'finish_reason': 'stop', 'logprobs': None}, id='run-a85b9f01-c803-4159-b31b-e76cbe902bd4-0', usage_metadata={'input_tokens': 74, 'output_tokens': 13, 'total_tokens': 87})]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input2 = \"what did i ask to you just now?\""
      ],
      "metadata": {
        "id": "2noFWSgwhhSu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_ai_message(input2)"
      ],
      "metadata": {
        "id": "1dl-TawYhojP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3FRWLDrhspW",
        "outputId": "334051a8-95d4-4dc8-fc1f-c7ddb1b92360"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='translate this given sentence from english to french: I LOVE AI.'),\n",
              " AIMessage(content=\"J'adore la AI.\"),\n",
              " AIMessage(content=\"J'adore la AI.\"),\n",
              " HumanMessage(content='who is the prime minister of india'),\n",
              " AIMessage(content='As of now, the Prime Minister of India is Narendra Modi.'),\n",
              " AIMessage(content='As of now, the Prime Minister of India is Narendra Modi.', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 74, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_3d37c73133', 'finish_reason': 'stop', 'logprobs': None}, id='run-a85b9f01-c803-4159-b31b-e76cbe902bd4-0', usage_metadata={'input_tokens': 74, 'output_tokens': 13, 'total_tokens': 87}),\n",
              " AIMessage(content='As of now, the Prime Minister of India is Narendra Modi.', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 74, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_3d37c73133', 'finish_reason': 'stop', 'logprobs': None}, id='run-a85b9f01-c803-4159-b31b-e76cbe902bd4-0', usage_metadata={'input_tokens': 74, 'output_tokens': 13, 'total_tokens': 87}),\n",
              " AIMessage(content='what did i ask to you just now?')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_chat_history.messages\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "od68v4MqiVJS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "fPA3SvDFieiC",
        "outputId": "4264b393-22e6-449d-94d2-e9fd7d5fecc6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You asked about the current Prime Minister of India.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "XtODI5LCigy7"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "U8Bgg-0HnMiF"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "8SBtWBGgnqnb"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "                              chain,\n",
        "                              lambda session_id: demo_chat_history,\n",
        "                              input_message_key= \"input\",\n",
        "                              history_message_key=\"chat_history\"\n",
        "\n",
        "                              )"
      ],
      "metadata": {
        "id": "kFRvdadWnyAa"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB3oUkRrq7Hv",
        "outputId": "13fd6f31-fe9d-4acf-aa83-483a3e29f7f0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
              "| RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='you are a helpful assistant. Answer all the question to the best of your ability.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
              "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79426cf3bd60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79426cb154b0>, model_name='gpt-3.5-turbo-1106', openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x79425fdc3d90>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function <lambda> at 0x79425fdc37f0>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Translate this sentence from English to French: I love programming.\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Pj5RCxn2rC",
        "outputId": "5dbe7af2-6a48-4a04-93c5-f1d099c3718c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.core:Parent run c9380b08-b770-4c8f-af5e-91d6204a96d8 not found for run 0c98de6e-6b67-4680-8f03-f7e7b0e08401. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='It looks like there was an error in processing your request. Could you please clarify your question or provide more information?', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 411, 'total_tokens': 434}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_3d37c73133', 'finish_reason': 'stop', 'logprobs': None}, id='run-81071e79-a1d1-4c47-9081-7c2a95958242-0', usage_metadata={'input_tokens': 411, 'output_tokens': 23, 'total_tokens': 434})"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2iAqLT9oH9p",
        "outputId": "2a1e043d-19b9-4d0a-9b5c-c1fea263b277"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.core:Parent run 6c9c7b98-cde5-420c-9a2b-157c57c6402e not found for run d203fd85-471f-4dda-98b6-73641f269984. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You asked me to translate the sentence \"I love programming\" from English to French. The translation for this sentence is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 577, 'total_tokens': 609}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_3d37c73133', 'finish_reason': 'stop', 'logprobs': None}, id='run-6f76b7ef-84fe-4090-95bc-1b4ff6bec1c8-0', usage_metadata={'input_tokens': 577, 'output_tokens': 32, 'total_tokens': 609})"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI"
      ],
      "metadata": {
        "id": "DxEBTR08sMYy"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''llm = OpenAI(\n",
        "\ttemperature=0,\n",
        " model_name=\"gpt-3.5-turbo-1106\"\n",
        ")\n",
        " '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "k5nKB17tsO_N",
        "outputId": "f3ae3e1e-bf30-40ca-fc54-e4e1bcd4752b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'llm = OpenAI(\\n\\ttemperature=0,\\n model_name=\"gpt-3.5-turbo-1106\"\\n)\\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain"
      ],
      "metadata": {
        "id": "RJ9SSOMEsRQV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm)"
      ],
      "metadata": {
        "id": "7HylENz4sToj"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBWJCM8gsXcf",
        "outputId": "823d6886-e190-41fe-c128-35f6b449add0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"I am interested in exploring the potential of large language models with external knowledge\""
      ],
      "metadata": {
        "id": "g6rogJoGsqr-"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = \"i would like to analyse all the different possibilities, what can you think of?\""
      ],
      "metadata": {
        "id": "oRGVfJsYszY9"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question3 = \"which data source tyoe could be used to give context to the model?\""
      ],
      "metadata": {
        "id": "3LrPSt4OtNGD"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "7WMtJUsFtX7z"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory= ConversationBufferMemory()\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "CoGLZYE1tuZF"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(\"good morning my AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNkgS9CdudTx",
        "outputId": "b0b6ea73-254e-432d-c333-b53f11ffba1a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'good morning my AI',\n",
              " 'history': '',\n",
              " 'response': \"Good morning! How are you today? I'm feeling great and ready to assist you with anything you need.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback"
      ],
      "metadata": {
        "id": "k3M9Vkdouf2w"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(chain,query):\n",
        "  with get_openai_callback() as cb:\n",
        "    result= chain.run(query)\n",
        "    print(f\"totla no of token is {cb.total_tokens}\")\n",
        "  return result"
      ],
      "metadata": {
        "id": "KJaOXSqqvNxx"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1=\"i am interested in exploring the potential of large language model with external knowledge\""
      ],
      "metadata": {
        "id": "9zQLQYBrvQCW"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(conversation_buf,question1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "4W5l-xsgvY3o",
        "outputId": "a0132044-6b87-4769-daac-872b9aa75a9d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totla no of token is 203\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's a fascinating topic! Large language models, like GPT-3, have the ability to generate human-like text and can be fine-tuned with external knowledge sources to enhance their understanding of specific topics. By integrating external knowledge, these models can provide more accurate and contextually relevant information. This can be particularly useful for tasks such as natural language understanding, question answering, and content generation. Is there a specific aspect of this topic you'd like to explore further?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question2=\"i would like to analysis all the different possibility, what can you think of?\""
      ],
      "metadata": {
        "id": "X5FMvPA1vcMB"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEX8xFm9wE5s",
        "outputId": "34ef3f42-e3d5-4810-aa8a-764c3dc67673"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'i would like to analysis all the different possibility, what can you think of?',\n",
              " 'history': \"Human: good morning my AI\\nAI: Good morning! How are you today? I'm feeling great and ready to assist you with anything you need.\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: That's a fascinating topic! Large language models, like GPT-3, have the ability to generate human-like text and can be fine-tuned with external knowledge sources to enhance their understanding of specific topics. By integrating external knowledge, these models can provide more accurate and contextually relevant information. This can be particularly useful for tasks such as natural language understanding, question answering, and content generation. Is there a specific aspect of this topic you'd like to explore further?\",\n",
              " 'response': \"There are numerous possibilities when it comes to integrating large language models with external knowledge. One possibility is leveraging structured knowledge bases, such as Wikipedia or DBpedia, to enrich the model's understanding of specific domains. Another possibility is using domain-specific knowledge graphs to enhance the model's contextual understanding of specialized topics. Additionally, integrating real-time data from sources like news articles or scientific publications can keep the model up-to-date with the latest information. Furthermore, incorporating user-generated content from platforms like Reddit or Quora can provide diverse perspectives on various topics. These are just a few examples, and the potential combinations of external knowledge sources are virtually endless. Is there a specific approach you're interested in exploring further, or any other possibilities you'd like to discuss?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(conversation_buf,question2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "hmm2cTaUwRqs",
        "outputId": "a25c45b2-8544-479d-97b9-664da18eafe1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totla no of token is 405\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, I cannot provide any more possibilities at this time.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "MH-5X2KUwUqz",
        "outputId": "4cac6176-2c6a-498f-840a-043cfcb5e172"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'which data source tyoe could be used to give context to the model?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question3)['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "kzaMBMobwXOd",
        "outputId": "cb008f28-b7c2-41ec-d09c-a3326975e4fc"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Some potential data sources that could be used to give context to the model include structured knowledge bases like Wikipedia or DBpedia, domain-specific knowledge graphs, real-time data from news articles or scientific publications, user-generated content from platforms like Reddit or Quora, and even proprietary data sources specific to certain industries or domains. Each of these data sources can provide valuable context and information to enhance the model's understanding of specific topics. Additionally, integrating multiple types of data sources can further enrich the model's contextual knowledge. Is there a specific type of data source you're interested in exploring further?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# take atleast 5 data source(1 ppt, 1 weblodaer, 1 pdf, 1txt, 1csv)\n",
        "\n",
        "# maintain one knowledge base(mongodb, astradb , pineconde, weviate)\n",
        "\n",
        "# user will ask you have to provide answer based on the asked question using this know;ledge base\n",
        "\n",
        "# handle the comman question as well like hi hello how are you good evening good morning etc.\n",
        "\n",
        "# you have to mention the complete memory of the conversation here the threshold is 10\n",
        "\n",
        "# then create a UI for you bot\n"
      ],
      "metadata": {
        "id": "SMh9tCpXyXE3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8jI1nr1jwiQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}